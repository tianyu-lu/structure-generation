{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tianyu-lu/structure-generation/blob/master/transformer-xl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otwi25i0vLdT",
        "colab_type": "text"
      },
      "source": [
        "## Huggingface Transformers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q3jBi1ycO4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
        "import glob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyFl6BMQvQ0o",
        "colab_type": "code",
        "outputId": "d429ed7e-0a44-49e2-afa4-e844cfe1039f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        }
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, TensorDataset\n",
        "from torch.autograd import Variable\n",
        "from transformers import GPT2LMHeadModel, GPT2Config, AdamW\n",
        "from transformers import TransfoXLLMHeadModel, TransfoXLConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "drive_name = '/content/drive'\n",
        "drive.mount(drive_name)\n",
        "drive_folder = ''\n",
        "drive_location = drive_name + '/My Drive/' + drive_folder  # Change this to where your files are located\n",
        "\n",
        "data_location = drive_location + '/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:transformers.file_utils:PyTorch version 1.5.0+cu101 available.\n",
            "INFO:transformers.file_utils:TensorFlow version 2.2.0 available.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24JCW-mtbzK5",
        "colab_type": "code",
        "outputId": "24039df1-dd2b-4b40-8f65-43957c893e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "%cd $data_location\n",
        "\n",
        "!pwd\n",
        "!cp /content/drive/'My Drive'/cath.tar.gz /content/\n",
        "%cd /content/\n",
        "!tar -xzf cath.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhmRInXuZh8f",
        "colab_type": "code",
        "outputId": "63da8927-99df-4227-fcfd-4901157e003d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seed_val = 777\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "def to_var(tensor):\n",
        "    return Variable(tensor)\n",
        "\n",
        "# for f in fnames:\n",
        "#     if f.endswith(\"npy\"):\n",
        "#         a = np.load(f, allow_pickle=True)\n",
        "#         length = int(a[0,0]*3)\n",
        "#         inputs.append((length, a[1:length+4]))\n",
        "# inputs.sort(key=lambda x: x[0])\n",
        "\n",
        "# use first n-1 to predict last n-1 by prepadding the short sequences\n",
        "# train_data = []\n",
        "# for i in range(batch_size-1, len(inputs), batch_size):\n",
        "#     train_batch = []\n",
        "#     max_len_in_batch = 1024\n",
        "#     for j in range(i-batch_size+1, i+1):\n",
        "#         curr_len = inputs[j][0] + 1\n",
        "#         if max_len_in_batch > curr_len:\n",
        "#             prepad = np.zeros((max_len_in_batch - curr_len, 4))\n",
        "#             # print(\"before:\", prepad.shape)\n",
        "#             # print(\"curr_len: \", curr_len)\n",
        "#             # print(\"actual len:\", len(inputs[j][1]))\n",
        "#             # print(inputs[j][1])\n",
        "#             prepad = np.concatenate((prepad, inputs[j][1]), axis=0)\n",
        "#             # print(\"after\", prepad.shape)\n",
        "#             # print(\"len(prepad)\", len(prepad))\n",
        "#             # print(\"max_len_in_batch\", max_len_in_batch)\n",
        "#             # print(\"\")\n",
        "#             assert len(prepad) == max_len_in_batch\n",
        "#             train_batch.append(prepad)\n",
        "#         elif max_len_in_batch == curr_len:\n",
        "#             assert len(inputs[j][1]) == max_len_in_batch\n",
        "#             train_batch.append(inputs[j][1])\n",
        "#         else:\n",
        "#             print(\"maxlen:\", max_len_in_batch)\n",
        "#             print(\"currlen:\", curr_len)\n",
        "#             raise ValueError\n",
        "#     train_data.append(train_batch)\n",
        "\n",
        "# use Transformer-XL to account for variable sequence lengths and contexts\n",
        "\n",
        "files = glob.glob(\"*.npy\")\n",
        "print(len(files))\n",
        "random.shuffle(files)\n",
        "split = int(0.8 * len(files))\n",
        "train_files = files[:split]\n",
        "val_files = files[split:]\n",
        "\n",
        "# when using the dataloader, just set batch_size=1, since we are already forming\n",
        "# the batches here\n",
        "batch_size = 8\n",
        "class ProteinDataset(Dataset):\n",
        "    def __init__(self, mode, batch_size=8):\n",
        "        self.batch_size = batch_size\n",
        "        if mode == 'train':\n",
        "            self.files = train_files\n",
        "        else:\n",
        "            self.files = val_files\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        #print(\"getting item\")\n",
        "        curr_batch = []\n",
        "        adj_idx = i % len(self.files)\n",
        "        fnames = self.files[adj_idx: adj_idx + self.batch_size]\n",
        "        i = 1\n",
        "        max_len = 0\n",
        "        while len(fnames) < self.batch_size:\n",
        "            fnames.append(self.files[adj_idx - i])\n",
        "            i += 1\n",
        "        #print(\"final fnames\", fnames)\n",
        "        for f in fnames:\n",
        "            try:\n",
        "                p = np.load(f)\n",
        "            except:\n",
        "                continue\n",
        "            curr_batch.append(p)\n",
        "            if len(p) > max_len:\n",
        "                max_len = len(p)\n",
        "        for i, p in enumerate(curr_batch):      # prepend with zeros\n",
        "            pad_len = max_len - len(p)\n",
        "            if pad_len > 0:\n",
        "                curr_batch[i] = np.concatenate((np.zeros((pad_len, 4)), p), axis=0)\n",
        "        return to_var(torch.FloatTensor(curr_batch))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "train_data = ProteinDataset('train', batch_size=batch_size)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=1)\n",
        "\n",
        "val_data = ProteinDataset('validation', batch_size=batch_size)\n",
        "val_sampler = RandomSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuJeNQ6eX9ji",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1866cdc1-efbf-4700-8ca8-afd6d1aeec41"
      },
      "source": [
        "print(len(train_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVnTLmdI1Ndx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  # def ce_and_mse(logits, coords, targets):\n",
        "#     t_id = targets[:, :, 0]\n",
        "\n",
        "#     logits = logits.view(-1,20)\n",
        "#     t_id = t_id.reshape((-1,1)).squeeze()\n",
        "\n",
        "#     t_id = t_id.type(torch.LongTensor)\n",
        "#     t_id = to_var(t_id)\n",
        "\n",
        "#     ce_func = nn.CrossEntropyLoss()\n",
        "#     ce_loss = ce_func(logits, t_id)\n",
        "#     mse_func = nn.MSELoss()\n",
        "#     mse_loss = mse_func(coords, targets[:, :, 1:])\n",
        "\n",
        "#     return ce_loss + mse_loss\n",
        "\n",
        "class XL_Protein(TransfoXLLMHeadModel):\n",
        "    def __init__(self, config):\n",
        "        super(XL_Protein, self).__init__(config)\n",
        "        \n",
        "        self.aa_layer = nn.Embedding(23, int(config.d_embed/2))  # may need to be Long\n",
        "        self.coord_layer = nn.Linear(3, int(config.d_embed/2))\n",
        "        self.coord_out = nn.Linear(config.d_model, 3, bias=False)\n",
        "\n",
        "    # labels only include the aa indices (including mask (-100), start, stop tokens)\n",
        "    def forward(self, input_ids=None, mems=None, head_mask=None, inputs_embeds=None, labels=None, coords=None):\n",
        "        bsz, tgt_len = input_ids.size(0), input_ids.size(1)\n",
        "        aa_embed = input_ids[:, :, 0].type(torch.LongTensor).squeeze().cuda()\n",
        "        aa_embed = self.aa_layer(aa_embed)\n",
        "        coord_embed = input_ids[:,:,1:].cuda()\n",
        "        coord_embed = self.coord_layer(coord_embed)\n",
        "        # print(\"aa_embed\", aa_embed.shape)\n",
        "        # print(\"coord_embed\", coord_embed.shape)\n",
        "        input_embeds = torch.cat((aa_embed, coord_embed), dim=2)\n",
        "        transformer_outputs = self.transformer(input_ids=None, mems=mems, head_mask=head_mask, inputs_embeds=input_embeds)\n",
        "\n",
        "        last_hidden = transformer_outputs[0]\n",
        "        pred_hid = last_hidden[:, -tgt_len:]\n",
        "        outputs = transformer_outputs[1:]\n",
        "\n",
        "        softmax_output = self.crit(pred_hid, labels)  # predicting aa\n",
        "        coord_pred = self.coord_out(pred_hid)       # predicting coords from the same hidden states (should this be split?)\n",
        "        mse = nn.MSELoss()\n",
        "        coords_loss = mse(coords, coord_pred)\n",
        "        #print(\"coords loss shape\", coords_loss.shape)\n",
        "\n",
        "        if labels is None:\n",
        "            softmax_output = softmax_output.view(bsz, tgt_len, -1)\n",
        "            outputs = [softmax_output, coord_pred] + outputs\n",
        "        else:\n",
        "            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n",
        "            loss =  softmax_output.mean() + coords_loss\n",
        "            outputs = [loss, coord_pred] + outputs\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class GPT_Protein(GPT2LMHeadModel):\n",
        "    def __init__(self, config):\n",
        "        super(GPT_Protein, self).__init__(config)\n",
        "\n",
        "        self.embedding = nn.Embedding(23, int(config.n_embd/2))  # may need to be Long\n",
        "\n",
        "        self.coord_layer = nn.Linear(3, int(config.n_embd/2))\n",
        "\n",
        "        self.lm_head_override = nn.Linear(config.n_embd, 23+3, bias=False)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "    ):\n",
        "        embed_in = input_ids[:, :, 0].type(torch.LongTensor).squeeze().cuda()\n",
        "        # print(\"trying to embed\")\n",
        "        # print(embed_in[0])\n",
        "        embed = self.embedding(embed_in).cuda()\n",
        "        coord_in = input_ids[:,:,1:].cuda()\n",
        "\n",
        "        self.coord_layer = self.coord_layer.cuda()\n",
        "        coord_in = self.coord_layer(coord_in)\n",
        "        #print(embed.shape)\n",
        "        #print(coord_in.shape)\n",
        "        input_embeds_transf = torch.cat((embed, coord_in), dim=2)\n",
        "        #print(input_embeds_transf.shape)\n",
        "\n",
        "        transformer_outputs = self.transformer(\n",
        "            input_ids=None,\n",
        "            past=past,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=input_embeds_transf,\n",
        "        )\n",
        "        hidden_states = transformer_outputs[0]\n",
        "        #print(\"hidden states: \", hidden_states.shape)\n",
        "        lm_out = self.lm_head_override(hidden_states)\n",
        "        #print(\"lm_out: \", lm_out.shape)\n",
        "        lm_logits = lm_out[:,:,:23]\n",
        "        lm_coords = lm_out[:,:,23:].cuda()\n",
        "        outputs = (lm_logits, lm_coords,) + transformer_outputs[1:]\n",
        "        if labels is not None:\n",
        "            real_aas = labels[:,:,0].squeeze()\n",
        "            real_coords = labels[:,:,1:].cuda()\n",
        "            # Shift so that tokens < n predict n\n",
        "            #print(\"lm_logits: \", lm_logits.shape)\n",
        "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
        "            #print(\"shift_logits: \", shift_logits.shape)\n",
        "            shift_labels = real_aas[..., 1:].contiguous()\n",
        "            # Flatten the tokens\n",
        "            ce_loss = nn.CrossEntropyLoss()\n",
        "            pred = shift_logits.view(-1, shift_logits.size(-1)).cuda()\n",
        "            target = shift_labels.view(-1)\n",
        "            target = target.type(torch.LongTensor).cuda()\n",
        "            # print(\"pred: \", pred)\n",
        "            # print(\"target: \", target)\n",
        "            loss = ce_loss(pred, target)\n",
        "            mse_loss = nn.MSELoss()\n",
        "            loss += mse_loss(lm_coords, real_coords)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "aa_to_int = {\"I\":1, \"V\":2, \"L\":3, \"F\":4, \"C\":5, \"M\":6, \"A\":7, \"G\":8, \"T\":9, \"S\":10,\n",
        "             \"W\":11, \"Y\":12, \"P\":13, \"H\":14, \"E\":15, \"Q\":16, \"D\":17, \"N\":18, \"K\":19, \"R\":20,\n",
        "             \"<START>\":21, \"<END>\":22, \"<PAD>\":0}\n",
        "idx_to_aa = {1:\"I\", 2:\"V\", 3:\"L\", 4:\"F\", 5:\"C\", 6:\"M\", 7:\"A\", 8:\"G\", 9:\"T\", 10:\"S\", \n",
        "             11:\"W\", 12:\"Y\", 13:\"P\", 14:\"H\", 15:\"E\", 16:\"Q\", 17:\"D\", 18:\"N\", 19:\"K\", 20:\"R\", \n",
        "             21:\"<START>\", 22:\"<END>\", 0:\"<PAD>\"}\n",
        "# for key in aa_to_int:\n",
        "#     print(str(aa_to_int[key]) + \":\\\"\" + key + \"\\\"\", end=', ')\n",
        "\n",
        "def train_model(model_type='XL'): \n",
        "\n",
        "    if model_type == 'XL':\n",
        "        config = TransfoXLConfig(vocab_size=23, cutoffs=[8, 13, 22],\n",
        "                                 d_model=64, d_embed=64, n_head=1, d_head=16, \n",
        "                                 d_inner=128, div_val=4, pre_lnorm=False, n_layer=3, \n",
        "                                 tgt_len=128, ext_len=0, mem_len=1600, clamp_len=1000, \n",
        "                                 same_length=False, proj_share_all_but_first=True, attn_type=0, \n",
        "                                 sample_softmax=- 1, adaptive=True, tie_weight=True, dropout=0.1, \n",
        "                                 dropatt=0.0, untie_r=True, init='normal', init_range=0.01, \n",
        "                                 proj_init_std=0.01, init_std=0.02, layer_norm_epsilon=1e-05, \n",
        "                                 eos_token_id=22)\n",
        "        model = XL_Protein(config)\n",
        "    else:\n",
        "        #model = GPT_Protein.from_pretrained(\"gpt2\")\n",
        "        config = GPT2Config(vocab_size=23, n_positions=1024, n_ctx=1024, n_embd=128, \n",
        "                  n_layer=4, n_head=4, activation_function='gelu_new', \n",
        "                  resid_pdrop=0.1, embd_pdrop=0.1, attn_pdrop=0.1, layer_norm_epsilon=1e-05, \n",
        "                  initializer_range=0.02, summary_type='cls_index', summary_use_proj=True, \n",
        "                  summary_activation=None, summary_proj_to_labels=True, summary_first_dropout=0.1, \n",
        "                  bos_token_id=21, eos_token_id=22)\n",
        "        model = GPT_Protein(config)\n",
        "\n",
        "    model.float()\n",
        "    model.cuda()   \n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "    total_steps = int(len(train_data) / batch_size)  # i.e. number of batches\n",
        "    # scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "    #                                         num_warmup_steps = 0, \n",
        "    #                                         num_training_steps = total_steps)\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "\n",
        "    num_epochs = 100\n",
        "    try:\n",
        "        for epoch_i in range(0, num_epochs):\n",
        "            print(\"\")\n",
        "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, num_epochs))\n",
        "            t0 = time.time()\n",
        "\n",
        "            total_loss = 0\n",
        "            model.train()\n",
        "\n",
        "            for i, b_input_ids in enumerate(train_dataloader):\n",
        "                # print(\"b_input_ids\", b_input_ids.shape)\n",
        "                b_input_ids = b_input_ids.squeeze(dim=0)\n",
        "                # b_input_ids = torch.FloatTensor(np.array(b_input_ids))\n",
        "                seq_len = b_input_ids.shape[1]\n",
        "                labels = b_input_ids[:,:,0].type(torch.LongTensor).squeeze().cuda()\n",
        "                coords = b_input_ids[:,:,1:].cuda()\n",
        "                outputs = model(input_ids=b_input_ids, mems=None, head_mask=None, inputs_embeds=None, labels=labels, coords=coords)\n",
        "\n",
        "                loss = outputs[0]\n",
        "                total_loss += loss.item() / seq_len\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                # scheduler.step()\n",
        "\n",
        "                if (i + 1) % 100 == 0:\n",
        "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
        "                        epoch_i + 1, \n",
        "                        num_epochs, \n",
        "                        i + 1, \n",
        "                        total_steps,\n",
        "                        loss.item())\n",
        "                      )\n",
        "\n",
        "            avg_train_loss = total_loss / total_steps * 100  # just for scaling purposes\n",
        "            train_loss.append(avg_train_loss)\n",
        "\n",
        "            print(\"\")\n",
        "            print(\"  Training epoch took: {:}\".format(format_time(time.time() - t0)))\n",
        "                \n",
        "\n",
        "            print(\"Running Validation...\")\n",
        "\n",
        "            # t0 = time.time()\n",
        "            model.eval()\n",
        "\n",
        "            eval_loss, eval_accuracy = 0, 0\n",
        "            best_loss = 10000\n",
        "            nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "            for i, b_input_ids in enumerate(val_dataloader):\n",
        "                b_input_ids = b_input_ids.squeeze(dim=0)\n",
        "                seq_len = b_input_ids.shape[1]\n",
        "                labels = b_input_ids[:,:,0].type(torch.LongTensor).squeeze().cuda()\n",
        "                coords = b_input_ids[:,:,1:].cuda()\n",
        "                with torch.no_grad(): \n",
        "                    outputs = model(input_ids=b_input_ids, mems=None, head_mask=None, inputs_embeds=None, labels=labels, coords=coords)\n",
        "                curr_val_loss, coord_pred, logits = outputs\n",
        "            #     # logits = logits.detach().cpu().numpy()\n",
        "            #     # coords = coords.detach().cpu().numpy()\n",
        "            #     # target_vals = b_labels.to('cpu').numpy()\n",
        "                \n",
        "                eval_loss += curr_val_loss.item() / seq_len\n",
        "                nb_eval_steps += 1\n",
        "\n",
        "            if eval_loss < best_loss:\n",
        "                best_loss = eval_loss\n",
        "                torch.save(model.state_dict(), \"best_model.pt\")\n",
        "            avg_val_loss = eval_loss/nb_eval_steps * 100  # for scaling\n",
        "            val_loss.append(avg_val_loss)\n",
        "            print(\"  Validation loss: {0:.2f}\".format(avg_val_loss))\n",
        "            # print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "        print(\"\")\n",
        "        print(\"Training complete.\")\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Early exit from training.\")\n",
        "        return train_loss, val_loss\n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def plot_loss(train_loss, val_loss):\n",
        "    sns.set(style='darkgrid')\n",
        "    sns.set(font_scale=2.5)\n",
        "    plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "    plt.plot(train_loss, 'b-o')\n",
        "    plt.plot(val_loss, 'r-o')\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b-6FqXs10he",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp best_model.pt /content/drive/'My Drive'/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpukJxYS-sEt",
        "colab_type": "code",
        "outputId": "194c5edd-c020-4614-ca4c-f86a696e3f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_loss, val_loss = train_model()\n",
        "plot_loss(train_loss, val_loss)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 100 ========\n",
            "Epoch [1/100], Step [100/3934.5], Loss: 2.6896\n",
            "Epoch [1/100], Step [200/3934.5], Loss: 2.7210\n",
            "Epoch [1/100], Step [300/3934.5], Loss: 1.7356\n",
            "Epoch [1/100], Step [400/3934.5], Loss: 1.2617\n",
            "Epoch [1/100], Step [500/3934.5], Loss: 2.1057\n",
            "Epoch [1/100], Step [600/3934.5], Loss: 1.9421\n",
            "Epoch [1/100], Step [700/3934.5], Loss: 1.3050\n",
            "Epoch [1/100], Step [800/3934.5], Loss: 1.0430\n",
            "Epoch [1/100], Step [900/3934.5], Loss: 1.3087\n",
            "Epoch [1/100], Step [1000/3934.5], Loss: 1.2361\n",
            "Epoch [1/100], Step [1100/3934.5], Loss: 0.6849\n",
            "Epoch [1/100], Step [1200/3934.5], Loss: 1.2391\n",
            "Epoch [1/100], Step [1300/3934.5], Loss: 1.1036\n",
            "Epoch [1/100], Step [1400/3934.5], Loss: 1.0907\n",
            "Epoch [1/100], Step [1500/3934.5], Loss: 0.7731\n",
            "Epoch [1/100], Step [1600/3934.5], Loss: 1.0367\n",
            "Epoch [1/100], Step [1700/3934.5], Loss: 0.5878\n",
            "Epoch [1/100], Step [1800/3934.5], Loss: 0.6031\n",
            "Epoch [1/100], Step [1900/3934.5], Loss: 1.1069\n",
            "Epoch [1/100], Step [2000/3934.5], Loss: 1.2622\n",
            "Epoch [1/100], Step [2100/3934.5], Loss: 0.5187\n",
            "Epoch [1/100], Step [2200/3934.5], Loss: 0.7389\n",
            "Epoch [1/100], Step [2300/3934.5], Loss: 0.8663\n",
            "Epoch [1/100], Step [2400/3934.5], Loss: 0.7465\n",
            "Epoch [1/100], Step [2500/3934.5], Loss: 0.7943\n",
            "Epoch [1/100], Step [2600/3934.5], Loss: 0.8933\n",
            "Epoch [1/100], Step [2700/3934.5], Loss: 0.7657\n",
            "Epoch [1/100], Step [2800/3934.5], Loss: 1.1005\n",
            "Epoch [1/100], Step [2900/3934.5], Loss: 0.4725\n",
            "Epoch [1/100], Step [3000/3934.5], Loss: 0.8432\n",
            "Epoch [1/100], Step [3100/3934.5], Loss: 0.7324\n",
            "Epoch [1/100], Step [3200/3934.5], Loss: 0.4156\n",
            "Epoch [1/100], Step [3300/3934.5], Loss: 0.7284\n",
            "Epoch [1/100], Step [3400/3934.5], Loss: 0.6174\n",
            "Epoch [1/100], Step [3500/3934.5], Loss: 0.8340\n",
            "Epoch [1/100], Step [3600/3934.5], Loss: 0.9886\n",
            "Epoch [1/100], Step [3700/3934.5], Loss: 0.9445\n",
            "Epoch [1/100], Step [3800/3934.5], Loss: 0.5113\n",
            "Epoch [1/100], Step [3900/3934.5], Loss: 0.6110\n",
            "Epoch [1/100], Step [4000/3934.5], Loss: 0.6373\n",
            "Epoch [1/100], Step [4100/3934.5], Loss: 0.7939\n",
            "Epoch [1/100], Step [4200/3934.5], Loss: 0.5647\n",
            "Epoch [1/100], Step [4300/3934.5], Loss: 0.5348\n",
            "Epoch [1/100], Step [4400/3934.5], Loss: 0.4589\n",
            "Epoch [1/100], Step [4500/3934.5], Loss: 1.1201\n",
            "Epoch [1/100], Step [4600/3934.5], Loss: 0.8442\n",
            "Epoch [1/100], Step [4700/3934.5], Loss: 0.5718\n",
            "Epoch [1/100], Step [4800/3934.5], Loss: 0.6960\n",
            "Epoch [1/100], Step [4900/3934.5], Loss: 0.6705\n",
            "Epoch [1/100], Step [5000/3934.5], Loss: 0.4484\n",
            "Epoch [1/100], Step [5100/3934.5], Loss: 0.7653\n",
            "Epoch [1/100], Step [5200/3934.5], Loss: 0.7093\n",
            "Epoch [1/100], Step [5300/3934.5], Loss: 0.6958\n",
            "Epoch [1/100], Step [5400/3934.5], Loss: 0.4844\n",
            "Epoch [1/100], Step [5500/3934.5], Loss: 0.9854\n",
            "Epoch [1/100], Step [5600/3934.5], Loss: 0.9512\n",
            "Epoch [1/100], Step [5700/3934.5], Loss: 0.6601\n",
            "Epoch [1/100], Step [5800/3934.5], Loss: 0.8865\n",
            "Epoch [1/100], Step [5900/3934.5], Loss: 0.6063\n",
            "Epoch [1/100], Step [6000/3934.5], Loss: 0.4691\n",
            "Epoch [1/100], Step [6100/3934.5], Loss: 0.5748\n",
            "Epoch [1/100], Step [6200/3934.5], Loss: 0.6130\n",
            "Epoch [1/100], Step [6300/3934.5], Loss: 0.3597\n",
            "Epoch [1/100], Step [6400/3934.5], Loss: 0.5341\n",
            "Epoch [1/100], Step [6500/3934.5], Loss: 0.7788\n",
            "Epoch [1/100], Step [6600/3934.5], Loss: 0.7759\n",
            "Epoch [1/100], Step [6700/3934.5], Loss: 0.6308\n",
            "Epoch [1/100], Step [6800/3934.5], Loss: 0.8493\n",
            "Epoch [1/100], Step [6900/3934.5], Loss: 0.5919\n",
            "Epoch [1/100], Step [7000/3934.5], Loss: 0.9254\n",
            "Epoch [1/100], Step [7100/3934.5], Loss: 0.6694\n",
            "Epoch [1/100], Step [7200/3934.5], Loss: 0.5565\n",
            "Epoch [1/100], Step [7300/3934.5], Loss: 0.7960\n",
            "Epoch [1/100], Step [7400/3934.5], Loss: 0.8464\n",
            "Epoch [1/100], Step [7500/3934.5], Loss: 0.6289\n",
            "Epoch [1/100], Step [7600/3934.5], Loss: 0.5167\n",
            "Epoch [1/100], Step [7700/3934.5], Loss: 0.8106\n",
            "Epoch [1/100], Step [7800/3934.5], Loss: 0.4030\n",
            "Epoch [1/100], Step [7900/3934.5], Loss: 0.9208\n",
            "Epoch [1/100], Step [8000/3934.5], Loss: 0.8763\n",
            "Epoch [1/100], Step [8100/3934.5], Loss: 0.7491\n",
            "Epoch [1/100], Step [8200/3934.5], Loss: 0.4849\n",
            "Epoch [1/100], Step [8300/3934.5], Loss: 1.0172\n",
            "Epoch [1/100], Step [8400/3934.5], Loss: 0.6687\n",
            "Epoch [1/100], Step [8500/3934.5], Loss: 0.4664\n",
            "Epoch [1/100], Step [8600/3934.5], Loss: 0.4185\n",
            "Epoch [1/100], Step [8700/3934.5], Loss: 0.7232\n",
            "Epoch [1/100], Step [8800/3934.5], Loss: 0.7826\n",
            "Epoch [1/100], Step [8900/3934.5], Loss: 0.6537\n",
            "Epoch [1/100], Step [9000/3934.5], Loss: 0.4979\n",
            "Epoch [1/100], Step [9100/3934.5], Loss: 0.4542\n",
            "Epoch [1/100], Step [9200/3934.5], Loss: 0.8592\n",
            "Epoch [1/100], Step [9300/3934.5], Loss: 0.8984\n",
            "Epoch [1/100], Step [9400/3934.5], Loss: 0.3704\n",
            "Epoch [1/100], Step [9500/3934.5], Loss: 0.7463\n",
            "Epoch [1/100], Step [9600/3934.5], Loss: 0.6221\n",
            "Epoch [1/100], Step [9700/3934.5], Loss: 0.5591\n",
            "Epoch [1/100], Step [9800/3934.5], Loss: 0.8579\n",
            "Epoch [1/100], Step [9900/3934.5], Loss: 0.6535\n",
            "Epoch [1/100], Step [10000/3934.5], Loss: 0.4443\n",
            "Epoch [1/100], Step [10100/3934.5], Loss: 0.4996\n",
            "Epoch [1/100], Step [10200/3934.5], Loss: 0.6706\n",
            "Epoch [1/100], Step [10300/3934.5], Loss: 0.3841\n",
            "Epoch [1/100], Step [10400/3934.5], Loss: 0.5321\n",
            "Epoch [1/100], Step [10500/3934.5], Loss: 0.4993\n",
            "Epoch [1/100], Step [10600/3934.5], Loss: 0.5298\n",
            "Epoch [1/100], Step [10700/3934.5], Loss: 0.5621\n",
            "Epoch [1/100], Step [10800/3934.5], Loss: 0.6506\n",
            "Epoch [1/100], Step [10900/3934.5], Loss: 0.5904\n",
            "Epoch [1/100], Step [11000/3934.5], Loss: 0.6279\n",
            "Epoch [1/100], Step [11100/3934.5], Loss: 0.6572\n",
            "Epoch [1/100], Step [11200/3934.5], Loss: 0.4158\n",
            "Epoch [1/100], Step [11300/3934.5], Loss: 0.4045\n",
            "Epoch [1/100], Step [11400/3934.5], Loss: 0.4694\n",
            "Epoch [1/100], Step [11500/3934.5], Loss: 0.3873\n",
            "Epoch [1/100], Step [11600/3934.5], Loss: 0.5556\n",
            "Epoch [1/100], Step [11700/3934.5], Loss: 0.7111\n",
            "Epoch [1/100], Step [11800/3934.5], Loss: 0.4493\n",
            "Epoch [1/100], Step [11900/3934.5], Loss: 0.5546\n",
            "Epoch [1/100], Step [12000/3934.5], Loss: 0.4715\n",
            "Epoch [1/100], Step [12100/3934.5], Loss: 0.7586\n",
            "Epoch [1/100], Step [12200/3934.5], Loss: 0.6725\n",
            "Epoch [1/100], Step [12300/3934.5], Loss: 0.6565\n",
            "Epoch [1/100], Step [12400/3934.5], Loss: 0.7670\n",
            "Epoch [1/100], Step [12500/3934.5], Loss: 0.5087\n",
            "Epoch [1/100], Step [12600/3934.5], Loss: 0.6362\n",
            "Epoch [1/100], Step [12700/3934.5], Loss: 0.6033\n",
            "Epoch [1/100], Step [12800/3934.5], Loss: 0.4307\n",
            "Epoch [1/100], Step [12900/3934.5], Loss: 0.6056\n",
            "Epoch [1/100], Step [13000/3934.5], Loss: 0.8871\n",
            "Epoch [1/100], Step [13100/3934.5], Loss: 0.3673\n",
            "Epoch [1/100], Step [13200/3934.5], Loss: 0.6691\n",
            "Epoch [1/100], Step [13300/3934.5], Loss: 0.3717\n",
            "Epoch [1/100], Step [13400/3934.5], Loss: 0.5384\n",
            "Epoch [1/100], Step [13500/3934.5], Loss: 0.4775\n",
            "Epoch [1/100], Step [13600/3934.5], Loss: 0.5593\n",
            "Epoch [1/100], Step [13700/3934.5], Loss: 0.7116\n",
            "Epoch [1/100], Step [13800/3934.5], Loss: 0.3068\n",
            "Epoch [1/100], Step [13900/3934.5], Loss: 0.5541\n",
            "Epoch [1/100], Step [14000/3934.5], Loss: 0.6587\n",
            "Epoch [1/100], Step [14100/3934.5], Loss: 0.6390\n",
            "Epoch [1/100], Step [14200/3934.5], Loss: 0.6220\n",
            "Epoch [1/100], Step [14300/3934.5], Loss: 0.3992\n",
            "Epoch [1/100], Step [14400/3934.5], Loss: 0.7703\n",
            "Epoch [1/100], Step [14500/3934.5], Loss: 0.6260\n",
            "Epoch [1/100], Step [14600/3934.5], Loss: 0.7311\n",
            "Epoch [1/100], Step [14700/3934.5], Loss: 0.6383\n",
            "Epoch [1/100], Step [14800/3934.5], Loss: 0.6235\n",
            "Epoch [1/100], Step [14900/3934.5], Loss: 0.7047\n",
            "Epoch [1/100], Step [15000/3934.5], Loss: 0.5937\n",
            "Epoch [1/100], Step [15100/3934.5], Loss: 0.3615\n",
            "Epoch [1/100], Step [15200/3934.5], Loss: 0.6528\n",
            "Epoch [1/100], Step [15300/3934.5], Loss: 0.4787\n",
            "Epoch [1/100], Step [15400/3934.5], Loss: 0.4800\n",
            "Epoch [1/100], Step [15500/3934.5], Loss: 0.5970\n",
            "Epoch [1/100], Step [15600/3934.5], Loss: 0.6696\n",
            "Epoch [1/100], Step [15700/3934.5], Loss: 0.5863\n",
            "Epoch [1/100], Step [15800/3934.5], Loss: 0.5763\n",
            "Epoch [1/100], Step [15900/3934.5], Loss: 0.5586\n",
            "Epoch [1/100], Step [16000/3934.5], Loss: 0.4309\n",
            "Epoch [1/100], Step [16100/3934.5], Loss: 0.7086\n",
            "Epoch [1/100], Step [16200/3934.5], Loss: 0.6571\n",
            "Epoch [1/100], Step [16300/3934.5], Loss: 0.4566\n",
            "Epoch [1/100], Step [16400/3934.5], Loss: 0.4827\n",
            "Epoch [1/100], Step [16500/3934.5], Loss: 0.5242\n",
            "Epoch [1/100], Step [16600/3934.5], Loss: 0.5886\n",
            "Epoch [1/100], Step [16700/3934.5], Loss: 0.4239\n",
            "Epoch [1/100], Step [16800/3934.5], Loss: 0.5589\n",
            "Epoch [1/100], Step [16900/3934.5], Loss: 0.3409\n",
            "Epoch [1/100], Step [17000/3934.5], Loss: 0.3335\n",
            "Epoch [1/100], Step [17100/3934.5], Loss: 0.4470\n",
            "Epoch [1/100], Step [17200/3934.5], Loss: 0.7918\n",
            "Epoch [1/100], Step [17300/3934.5], Loss: 0.6287\n",
            "Epoch [1/100], Step [17400/3934.5], Loss: 0.6759\n",
            "Epoch [1/100], Step [17500/3934.5], Loss: 0.3323\n",
            "Epoch [1/100], Step [17600/3934.5], Loss: 0.5419\n",
            "Epoch [1/100], Step [17700/3934.5], Loss: 0.7386\n",
            "Epoch [1/100], Step [17800/3934.5], Loss: 0.6990\n",
            "Epoch [1/100], Step [17900/3934.5], Loss: 0.6292\n",
            "Epoch [1/100], Step [18000/3934.5], Loss: 0.4040\n",
            "Epoch [1/100], Step [18100/3934.5], Loss: 0.3180\n",
            "Epoch [1/100], Step [18200/3934.5], Loss: 0.2683\n",
            "Epoch [1/100], Step [18300/3934.5], Loss: 0.5410\n",
            "Epoch [1/100], Step [18400/3934.5], Loss: 0.7994\n",
            "Epoch [1/100], Step [18500/3934.5], Loss: 0.7116\n",
            "Epoch [1/100], Step [18600/3934.5], Loss: 0.4877\n",
            "Epoch [1/100], Step [18700/3934.5], Loss: 0.5891\n",
            "Epoch [1/100], Step [18800/3934.5], Loss: 0.7709\n",
            "Epoch [1/100], Step [18900/3934.5], Loss: 0.7227\n",
            "Epoch [1/100], Step [19000/3934.5], Loss: 0.6613\n",
            "Epoch [1/100], Step [19100/3934.5], Loss: 0.3721\n",
            "Epoch [1/100], Step [19200/3934.5], Loss: 0.3250\n",
            "Epoch [1/100], Step [19300/3934.5], Loss: 0.7000\n",
            "Epoch [1/100], Step [19400/3934.5], Loss: 0.4766\n",
            "Epoch [1/100], Step [19500/3934.5], Loss: 0.4691\n",
            "Epoch [1/100], Step [19600/3934.5], Loss: 0.4552\n",
            "Epoch [1/100], Step [19700/3934.5], Loss: 0.5867\n",
            "Epoch [1/100], Step [19800/3934.5], Loss: 0.3237\n",
            "Epoch [1/100], Step [19900/3934.5], Loss: 0.6285\n",
            "Epoch [1/100], Step [20000/3934.5], Loss: 0.4518\n",
            "Epoch [1/100], Step [20100/3934.5], Loss: 0.6406\n",
            "Epoch [1/100], Step [20200/3934.5], Loss: 0.3629\n",
            "Epoch [1/100], Step [20300/3934.5], Loss: 0.5132\n",
            "Epoch [1/100], Step [20400/3934.5], Loss: 0.4216\n",
            "Epoch [1/100], Step [20500/3934.5], Loss: 0.3779\n",
            "Epoch [1/100], Step [20600/3934.5], Loss: 0.2960\n",
            "Epoch [1/100], Step [20700/3934.5], Loss: 0.4253\n",
            "Epoch [1/100], Step [20800/3934.5], Loss: 0.5355\n",
            "Epoch [1/100], Step [20900/3934.5], Loss: 0.5046\n",
            "Epoch [1/100], Step [21000/3934.5], Loss: 0.6003\n",
            "Epoch [1/100], Step [21100/3934.5], Loss: 0.6307\n",
            "Epoch [1/100], Step [21200/3934.5], Loss: 0.5903\n",
            "Epoch [1/100], Step [21300/3934.5], Loss: 0.3274\n",
            "Epoch [1/100], Step [21400/3934.5], Loss: 0.5595\n",
            "Epoch [1/100], Step [21500/3934.5], Loss: 0.4580\n",
            "Epoch [1/100], Step [21600/3934.5], Loss: 0.4267\n",
            "Epoch [1/100], Step [21700/3934.5], Loss: 0.5414\n",
            "Epoch [1/100], Step [21800/3934.5], Loss: 0.4379\n",
            "Epoch [1/100], Step [21900/3934.5], Loss: 0.4237\n",
            "Epoch [1/100], Step [22000/3934.5], Loss: 0.4490\n",
            "Epoch [1/100], Step [22100/3934.5], Loss: 0.4589\n",
            "Epoch [1/100], Step [22200/3934.5], Loss: 0.7311\n",
            "Epoch [1/100], Step [22300/3934.5], Loss: 0.6385\n",
            "Epoch [1/100], Step [22400/3934.5], Loss: 0.5631\n",
            "Epoch [1/100], Step [22500/3934.5], Loss: 0.4903\n",
            "Epoch [1/100], Step [22600/3934.5], Loss: 0.4686\n",
            "Epoch [1/100], Step [22700/3934.5], Loss: 0.5790\n",
            "Epoch [1/100], Step [22800/3934.5], Loss: 0.5941\n",
            "Epoch [1/100], Step [22900/3934.5], Loss: 0.3806\n",
            "Epoch [1/100], Step [23000/3934.5], Loss: 0.6656\n",
            "Epoch [1/100], Step [23100/3934.5], Loss: 0.4520\n",
            "Epoch [1/100], Step [23200/3934.5], Loss: 0.6819\n",
            "Epoch [1/100], Step [23300/3934.5], Loss: 0.5137\n",
            "Epoch [1/100], Step [23400/3934.5], Loss: 0.2980\n",
            "Epoch [1/100], Step [23500/3934.5], Loss: 0.7258\n",
            "Epoch [1/100], Step [23600/3934.5], Loss: 0.3249\n",
            "Epoch [1/100], Step [23700/3934.5], Loss: 0.3018\n",
            "Epoch [1/100], Step [23800/3934.5], Loss: 0.4596\n",
            "Epoch [1/100], Step [23900/3934.5], Loss: 0.4446\n",
            "Epoch [1/100], Step [24000/3934.5], Loss: 0.3351\n",
            "Epoch [1/100], Step [24100/3934.5], Loss: 0.6392\n",
            "Epoch [1/100], Step [24200/3934.5], Loss: 0.5284\n",
            "Epoch [1/100], Step [24300/3934.5], Loss: 0.2630\n",
            "Epoch [1/100], Step [24400/3934.5], Loss: 0.5432\n",
            "Epoch [1/100], Step [24500/3934.5], Loss: 0.4552\n",
            "Epoch [1/100], Step [24600/3934.5], Loss: 0.6890\n",
            "Epoch [1/100], Step [24700/3934.5], Loss: 0.5809\n",
            "Epoch [1/100], Step [24800/3934.5], Loss: 0.6926\n",
            "Epoch [1/100], Step [24900/3934.5], Loss: 0.4768\n",
            "Epoch [1/100], Step [25000/3934.5], Loss: 0.3439\n",
            "Epoch [1/100], Step [25100/3934.5], Loss: 0.3786\n",
            "Epoch [1/100], Step [25200/3934.5], Loss: 0.5970\n",
            "Epoch [1/100], Step [25300/3934.5], Loss: 0.5630\n",
            "Epoch [1/100], Step [25400/3934.5], Loss: 0.4279\n",
            "Epoch [1/100], Step [25500/3934.5], Loss: 0.4357\n",
            "Epoch [1/100], Step [25600/3934.5], Loss: 0.5242\n",
            "Epoch [1/100], Step [25700/3934.5], Loss: 0.7846\n",
            "Epoch [1/100], Step [25800/3934.5], Loss: 0.4326\n",
            "Epoch [1/100], Step [25900/3934.5], Loss: 0.5036\n",
            "Epoch [1/100], Step [26000/3934.5], Loss: 0.4578\n",
            "Epoch [1/100], Step [26100/3934.5], Loss: 0.4740\n",
            "Epoch [1/100], Step [26200/3934.5], Loss: 0.4404\n",
            "Epoch [1/100], Step [26300/3934.5], Loss: 0.3938\n",
            "Epoch [1/100], Step [26400/3934.5], Loss: 0.7428\n",
            "Epoch [1/100], Step [26500/3934.5], Loss: 0.4042\n",
            "Epoch [1/100], Step [26600/3934.5], Loss: 0.9940\n",
            "Epoch [1/100], Step [26700/3934.5], Loss: 0.4471\n",
            "Epoch [1/100], Step [26800/3934.5], Loss: 0.4195\n",
            "Epoch [1/100], Step [26900/3934.5], Loss: 0.6073\n",
            "Epoch [1/100], Step [27000/3934.5], Loss: 0.5695\n",
            "Epoch [1/100], Step [27100/3934.5], Loss: 0.3013\n",
            "Epoch [1/100], Step [27200/3934.5], Loss: 0.5292\n",
            "Epoch [1/100], Step [27300/3934.5], Loss: 0.4961\n",
            "Epoch [1/100], Step [27400/3934.5], Loss: 0.3146\n",
            "Epoch [1/100], Step [27500/3934.5], Loss: 0.7515\n",
            "Epoch [1/100], Step [27600/3934.5], Loss: 0.6798\n",
            "Epoch [1/100], Step [27700/3934.5], Loss: 0.3203\n",
            "Epoch [1/100], Step [27800/3934.5], Loss: 0.5966\n",
            "Epoch [1/100], Step [27900/3934.5], Loss: 0.5115\n",
            "Epoch [1/100], Step [28000/3934.5], Loss: 0.5041\n",
            "Epoch [1/100], Step [28100/3934.5], Loss: 0.3806\n",
            "Epoch [1/100], Step [28200/3934.5], Loss: 0.4187\n",
            "Epoch [1/100], Step [28300/3934.5], Loss: 0.7594\n",
            "Epoch [1/100], Step [28400/3934.5], Loss: 0.7833\n",
            "Epoch [1/100], Step [28500/3934.5], Loss: 0.6551\n",
            "Epoch [1/100], Step [28600/3934.5], Loss: 0.4940\n",
            "Epoch [1/100], Step [28700/3934.5], Loss: 0.6506\n",
            "Epoch [1/100], Step [28800/3934.5], Loss: 0.5835\n",
            "Epoch [1/100], Step [28900/3934.5], Loss: 0.5219\n",
            "Epoch [1/100], Step [29000/3934.5], Loss: 0.4232\n",
            "Epoch [1/100], Step [29100/3934.5], Loss: 0.7104\n",
            "Epoch [1/100], Step [29200/3934.5], Loss: 0.5484\n",
            "Epoch [1/100], Step [29300/3934.5], Loss: 0.4875\n",
            "Epoch [1/100], Step [29400/3934.5], Loss: 0.6319\n",
            "Epoch [1/100], Step [29500/3934.5], Loss: 0.6299\n",
            "Epoch [1/100], Step [29600/3934.5], Loss: 0.6336\n",
            "Epoch [1/100], Step [29700/3934.5], Loss: 0.6898\n",
            "Epoch [1/100], Step [29800/3934.5], Loss: 0.3844\n",
            "Epoch [1/100], Step [29900/3934.5], Loss: 0.5085\n",
            "Epoch [1/100], Step [30000/3934.5], Loss: 0.6425\n",
            "Epoch [1/100], Step [30100/3934.5], Loss: 0.3149\n",
            "Epoch [1/100], Step [30200/3934.5], Loss: 0.5228\n",
            "Epoch [1/100], Step [30300/3934.5], Loss: 0.4305\n",
            "Epoch [1/100], Step [30400/3934.5], Loss: 0.5680\n",
            "Epoch [1/100], Step [30500/3934.5], Loss: 0.5370\n",
            "Epoch [1/100], Step [30600/3934.5], Loss: 0.5774\n",
            "Epoch [1/100], Step [30700/3934.5], Loss: 0.2743\n",
            "Epoch [1/100], Step [30800/3934.5], Loss: 0.6728\n",
            "Epoch [1/100], Step [30900/3934.5], Loss: 0.6412\n",
            "Epoch [1/100], Step [31000/3934.5], Loss: 0.6413\n",
            "Epoch [1/100], Step [31100/3934.5], Loss: 0.4578\n",
            "Epoch [1/100], Step [31200/3934.5], Loss: 0.5854\n",
            "Epoch [1/100], Step [31300/3934.5], Loss: 0.6232\n",
            "Epoch [1/100], Step [31400/3934.5], Loss: 0.5943\n",
            "\n",
            "  Training epoch took: 2:40:31\n",
            "Running Validation...\n",
            "  Validation loss: 0.06\n",
            "\n",
            "======== Epoch 2 / 100 ========\n",
            "Epoch [2/100], Step [100/3934.5], Loss: 0.6786\n",
            "Epoch [2/100], Step [200/3934.5], Loss: 0.4083\n",
            "Epoch [2/100], Step [300/3934.5], Loss: 0.5038\n",
            "Epoch [2/100], Step [400/3934.5], Loss: 0.4888\n",
            "Epoch [2/100], Step [500/3934.5], Loss: 0.6080\n",
            "Epoch [2/100], Step [600/3934.5], Loss: 0.3707\n",
            "Epoch [2/100], Step [700/3934.5], Loss: 0.4479\n",
            "Epoch [2/100], Step [800/3934.5], Loss: 0.3795\n",
            "Epoch [2/100], Step [900/3934.5], Loss: 0.4500\n",
            "Epoch [2/100], Step [1000/3934.5], Loss: 0.4259\n",
            "Epoch [2/100], Step [1100/3934.5], Loss: 0.6116\n",
            "Epoch [2/100], Step [1200/3934.5], Loss: 0.6918\n",
            "Epoch [2/100], Step [1300/3934.5], Loss: 0.6060\n",
            "Epoch [2/100], Step [1400/3934.5], Loss: 0.4966\n",
            "Epoch [2/100], Step [1500/3934.5], Loss: 0.4054\n",
            "Epoch [2/100], Step [1600/3934.5], Loss: 0.6571\n",
            "Epoch [2/100], Step [1700/3934.5], Loss: 0.3949\n",
            "Epoch [2/100], Step [1800/3934.5], Loss: 0.4461\n",
            "Epoch [2/100], Step [1900/3934.5], Loss: 0.4391\n",
            "Epoch [2/100], Step [2000/3934.5], Loss: 0.4197\n",
            "Epoch [2/100], Step [2100/3934.5], Loss: 0.4544\n",
            "Epoch [2/100], Step [2200/3934.5], Loss: 0.6916\n",
            "Epoch [2/100], Step [2300/3934.5], Loss: 0.5006\n",
            "Epoch [2/100], Step [2400/3934.5], Loss: 0.5395\n",
            "Epoch [2/100], Step [2500/3934.5], Loss: 0.3361\n",
            "Epoch [2/100], Step [2600/3934.5], Loss: 0.4858\n",
            "Epoch [2/100], Step [2700/3934.5], Loss: 0.7289\n",
            "Epoch [2/100], Step [2800/3934.5], Loss: 0.3616\n",
            "Epoch [2/100], Step [2900/3934.5], Loss: 0.2904\n",
            "Epoch [2/100], Step [3000/3934.5], Loss: 0.4686\n",
            "Epoch [2/100], Step [3100/3934.5], Loss: 0.3435\n",
            "Epoch [2/100], Step [3200/3934.5], Loss: 0.3870\n",
            "Epoch [2/100], Step [3300/3934.5], Loss: 0.6118\n",
            "Epoch [2/100], Step [3400/3934.5], Loss: 0.3810\n",
            "Epoch [2/100], Step [3500/3934.5], Loss: 0.6201\n",
            "Epoch [2/100], Step [3600/3934.5], Loss: 0.5354\n",
            "Epoch [2/100], Step [3700/3934.5], Loss: 0.3667\n",
            "Epoch [2/100], Step [3800/3934.5], Loss: 0.7956\n",
            "Epoch [2/100], Step [3900/3934.5], Loss: 0.6155\n",
            "Epoch [2/100], Step [4000/3934.5], Loss: 0.7303\n",
            "Epoch [2/100], Step [4100/3934.5], Loss: 0.6373\n",
            "Epoch [2/100], Step [4200/3934.5], Loss: 0.7195\n",
            "Epoch [2/100], Step [4300/3934.5], Loss: 0.7731\n",
            "Epoch [2/100], Step [4400/3934.5], Loss: 0.6100\n",
            "Epoch [2/100], Step [4500/3934.5], Loss: 0.6127\n",
            "Epoch [2/100], Step [4600/3934.5], Loss: 0.4860\n",
            "Epoch [2/100], Step [4700/3934.5], Loss: 0.4259\n",
            "Epoch [2/100], Step [4800/3934.5], Loss: 0.4814\n",
            "Epoch [2/100], Step [4900/3934.5], Loss: 0.7231\n",
            "Epoch [2/100], Step [5000/3934.5], Loss: 0.4839\n",
            "Epoch [2/100], Step [5100/3934.5], Loss: 0.5059\n",
            "Epoch [2/100], Step [5200/3934.5], Loss: 0.5450\n",
            "Epoch [2/100], Step [5300/3934.5], Loss: 0.4601\n",
            "Epoch [2/100], Step [5400/3934.5], Loss: 0.5889\n",
            "Epoch [2/100], Step [5500/3934.5], Loss: 0.7156\n",
            "Epoch [2/100], Step [5600/3934.5], Loss: 0.3961\n",
            "Epoch [2/100], Step [5700/3934.5], Loss: 0.5621\n",
            "Epoch [2/100], Step [5800/3934.5], Loss: 0.5554\n",
            "Epoch [2/100], Step [5900/3934.5], Loss: 0.6295\n",
            "Epoch [2/100], Step [6000/3934.5], Loss: 0.5384\n",
            "Epoch [2/100], Step [6100/3934.5], Loss: 0.5361\n",
            "Epoch [2/100], Step [6200/3934.5], Loss: 0.5747\n",
            "Epoch [2/100], Step [6300/3934.5], Loss: 0.4663\n",
            "Epoch [2/100], Step [6400/3934.5], Loss: 0.5972\n",
            "Epoch [2/100], Step [6500/3934.5], Loss: 0.5698\n",
            "Epoch [2/100], Step [6600/3934.5], Loss: 0.5926\n",
            "Epoch [2/100], Step [6700/3934.5], Loss: 0.4213\n",
            "Epoch [2/100], Step [6800/3934.5], Loss: 0.6860\n",
            "Epoch [2/100], Step [6900/3934.5], Loss: 0.4239\n",
            "Epoch [2/100], Step [7000/3934.5], Loss: 0.4366\n",
            "Epoch [2/100], Step [7100/3934.5], Loss: 0.4009\n",
            "Epoch [2/100], Step [7200/3934.5], Loss: 0.5026\n",
            "Epoch [2/100], Step [7300/3934.5], Loss: 0.5758\n",
            "Epoch [2/100], Step [7400/3934.5], Loss: 0.5553\n",
            "Epoch [2/100], Step [7500/3934.5], Loss: 0.5746\n",
            "Epoch [2/100], Step [7600/3934.5], Loss: 0.4577\n",
            "Epoch [2/100], Step [7700/3934.5], Loss: 0.3637\n",
            "Epoch [2/100], Step [7800/3934.5], Loss: 0.3802\n",
            "Epoch [2/100], Step [7900/3934.5], Loss: 0.4953\n",
            "Epoch [2/100], Step [8000/3934.5], Loss: 0.4064\n",
            "Epoch [2/100], Step [8100/3934.5], Loss: 0.6009\n",
            "Epoch [2/100], Step [8200/3934.5], Loss: 0.2998\n",
            "Epoch [2/100], Step [8300/3934.5], Loss: 0.6877\n",
            "Epoch [2/100], Step [8400/3934.5], Loss: 0.3490\n",
            "Epoch [2/100], Step [8500/3934.5], Loss: 0.3335\n",
            "Epoch [2/100], Step [8600/3934.5], Loss: 0.3265\n",
            "Epoch [2/100], Step [8700/3934.5], Loss: 0.3760\n",
            "Epoch [2/100], Step [8800/3934.5], Loss: 0.5483\n",
            "Epoch [2/100], Step [8900/3934.5], Loss: 0.6950\n",
            "Epoch [2/100], Step [9000/3934.5], Loss: 0.4461\n",
            "Epoch [2/100], Step [9100/3934.5], Loss: 0.3719\n",
            "Epoch [2/100], Step [9200/3934.5], Loss: 0.4733\n",
            "Epoch [2/100], Step [9300/3934.5], Loss: 0.2811\n",
            "Epoch [2/100], Step [9400/3934.5], Loss: 0.4750\n",
            "Epoch [2/100], Step [9500/3934.5], Loss: 0.7321\n",
            "Epoch [2/100], Step [9600/3934.5], Loss: 0.6853\n",
            "Epoch [2/100], Step [9700/3934.5], Loss: 0.4164\n",
            "Epoch [2/100], Step [9800/3934.5], Loss: 0.3919\n",
            "Epoch [2/100], Step [9900/3934.5], Loss: 0.3837\n",
            "Epoch [2/100], Step [10000/3934.5], Loss: 0.3936\n",
            "Epoch [2/100], Step [10100/3934.5], Loss: 0.7104\n",
            "Epoch [2/100], Step [10200/3934.5], Loss: 0.6330\n",
            "Epoch [2/100], Step [10300/3934.5], Loss: 0.4827\n",
            "Epoch [2/100], Step [10400/3934.5], Loss: 0.4560\n",
            "Epoch [2/100], Step [10500/3934.5], Loss: 0.4144\n",
            "Epoch [2/100], Step [10600/3934.5], Loss: 0.3414\n",
            "Epoch [2/100], Step [10700/3934.5], Loss: 0.4357\n",
            "Epoch [2/100], Step [10800/3934.5], Loss: 0.3923\n",
            "Epoch [2/100], Step [10900/3934.5], Loss: 0.4734\n",
            "Epoch [2/100], Step [11000/3934.5], Loss: 0.3867\n",
            "Epoch [2/100], Step [11100/3934.5], Loss: 0.4080\n",
            "Epoch [2/100], Step [11200/3934.5], Loss: 0.4699\n",
            "Epoch [2/100], Step [11300/3934.5], Loss: 0.6855\n",
            "Epoch [2/100], Step [11400/3934.5], Loss: 0.5321\n",
            "Epoch [2/100], Step [11500/3934.5], Loss: 0.5310\n",
            "Epoch [2/100], Step [11600/3934.5], Loss: 0.4932\n",
            "Epoch [2/100], Step [11700/3934.5], Loss: 0.5095\n",
            "Epoch [2/100], Step [11800/3934.5], Loss: 0.4880\n",
            "Epoch [2/100], Step [11900/3934.5], Loss: 0.3013\n",
            "Epoch [2/100], Step [12000/3934.5], Loss: 0.5913\n",
            "Epoch [2/100], Step [12100/3934.5], Loss: 0.5774\n",
            "Epoch [2/100], Step [12200/3934.5], Loss: 0.5256\n",
            "Epoch [2/100], Step [12300/3934.5], Loss: 0.5158\n",
            "Epoch [2/100], Step [12400/3934.5], Loss: 0.4839\n",
            "Epoch [2/100], Step [12500/3934.5], Loss: 0.5631\n",
            "Epoch [2/100], Step [12600/3934.5], Loss: 0.3615\n",
            "Epoch [2/100], Step [12700/3934.5], Loss: 0.5595\n",
            "Epoch [2/100], Step [12800/3934.5], Loss: 0.5637\n",
            "Epoch [2/100], Step [12900/3934.5], Loss: 0.5887\n",
            "Epoch [2/100], Step [13000/3934.5], Loss: 0.4861\n",
            "Epoch [2/100], Step [13100/3934.5], Loss: 0.6343\n",
            "Epoch [2/100], Step [13200/3934.5], Loss: 0.5589\n",
            "Epoch [2/100], Step [13300/3934.5], Loss: 0.4834\n",
            "Epoch [2/100], Step [13400/3934.5], Loss: 0.7843\n",
            "Epoch [2/100], Step [13500/3934.5], Loss: 0.4905\n",
            "Epoch [2/100], Step [13600/3934.5], Loss: 0.5680\n",
            "Epoch [2/100], Step [13700/3934.5], Loss: 0.4212\n",
            "Epoch [2/100], Step [13800/3934.5], Loss: 0.4367\n",
            "Epoch [2/100], Step [13900/3934.5], Loss: 0.3556\n",
            "Epoch [2/100], Step [14000/3934.5], Loss: 0.4720\n",
            "Epoch [2/100], Step [14100/3934.5], Loss: 0.6533\n",
            "Epoch [2/100], Step [14200/3934.5], Loss: 0.2853\n",
            "Epoch [2/100], Step [14300/3934.5], Loss: 0.3009\n",
            "Epoch [2/100], Step [14400/3934.5], Loss: 0.4243\n",
            "Epoch [2/100], Step [14500/3934.5], Loss: 0.4953\n",
            "Epoch [2/100], Step [14600/3934.5], Loss: 0.5986\n",
            "Epoch [2/100], Step [14700/3934.5], Loss: 0.6295\n",
            "Epoch [2/100], Step [14800/3934.5], Loss: 0.5492\n",
            "Epoch [2/100], Step [14900/3934.5], Loss: 0.8782\n",
            "Epoch [2/100], Step [15000/3934.5], Loss: 0.3638\n",
            "Epoch [2/100], Step [15100/3934.5], Loss: 0.5009\n",
            "Epoch [2/100], Step [15200/3934.5], Loss: 0.3978\n",
            "Epoch [2/100], Step [15300/3934.5], Loss: 0.5937\n",
            "Epoch [2/100], Step [15400/3934.5], Loss: 0.3470\n",
            "Epoch [2/100], Step [15500/3934.5], Loss: 0.3823\n",
            "Epoch [2/100], Step [15600/3934.5], Loss: 0.5372\n",
            "Epoch [2/100], Step [15700/3934.5], Loss: 0.5738\n",
            "Epoch [2/100], Step [15800/3934.5], Loss: 0.4995\n",
            "Epoch [2/100], Step [15900/3934.5], Loss: 0.5156\n",
            "Epoch [2/100], Step [16000/3934.5], Loss: 0.7941\n",
            "Epoch [2/100], Step [16100/3934.5], Loss: 0.5864\n",
            "Epoch [2/100], Step [16200/3934.5], Loss: 0.6674\n",
            "Epoch [2/100], Step [16300/3934.5], Loss: 0.6086\n",
            "Epoch [2/100], Step [16400/3934.5], Loss: 0.7068\n",
            "Epoch [2/100], Step [16500/3934.5], Loss: 0.4283\n",
            "Epoch [2/100], Step [16600/3934.5], Loss: 0.4109\n",
            "Epoch [2/100], Step [16700/3934.5], Loss: 0.5746\n",
            "Epoch [2/100], Step [16800/3934.5], Loss: 0.6219\n",
            "Epoch [2/100], Step [16900/3934.5], Loss: 0.7391\n",
            "Epoch [2/100], Step [17000/3934.5], Loss: 0.5775\n",
            "Epoch [2/100], Step [17100/3934.5], Loss: 0.6700\n",
            "Epoch [2/100], Step [17200/3934.5], Loss: 0.3890\n",
            "Epoch [2/100], Step [17300/3934.5], Loss: 0.4148\n",
            "Epoch [2/100], Step [17400/3934.5], Loss: 0.4981\n",
            "Epoch [2/100], Step [17500/3934.5], Loss: 0.4881\n",
            "Epoch [2/100], Step [17600/3934.5], Loss: 0.2939\n",
            "Epoch [2/100], Step [17700/3934.5], Loss: 0.2815\n",
            "Epoch [2/100], Step [17800/3934.5], Loss: 0.2840\n",
            "Epoch [2/100], Step [17900/3934.5], Loss: 0.5328\n",
            "Epoch [2/100], Step [18000/3934.5], Loss: 0.6739\n",
            "Epoch [2/100], Step [18100/3934.5], Loss: 0.5349\n",
            "Epoch [2/100], Step [18200/3934.5], Loss: 0.4999\n",
            "Epoch [2/100], Step [18300/3934.5], Loss: 0.3748\n",
            "Epoch [2/100], Step [18400/3934.5], Loss: 0.3105\n",
            "Epoch [2/100], Step [18500/3934.5], Loss: 0.3529\n",
            "Epoch [2/100], Step [18600/3934.5], Loss: 0.3378\n",
            "Epoch [2/100], Step [18700/3934.5], Loss: 0.4145\n",
            "Epoch [2/100], Step [18800/3934.5], Loss: 0.3988\n",
            "Epoch [2/100], Step [18900/3934.5], Loss: 0.5829\n",
            "Epoch [2/100], Step [19000/3934.5], Loss: 0.3297\n",
            "Epoch [2/100], Step [19100/3934.5], Loss: 0.5254\n",
            "Epoch [2/100], Step [19200/3934.5], Loss: 0.3423\n",
            "Epoch [2/100], Step [19300/3934.5], Loss: 0.6520\n",
            "Epoch [2/100], Step [19400/3934.5], Loss: 0.4971\n",
            "Epoch [2/100], Step [19500/3934.5], Loss: 0.4701\n",
            "Epoch [2/100], Step [19600/3934.5], Loss: 0.3485\n",
            "Epoch [2/100], Step [19700/3934.5], Loss: 0.3342\n",
            "Epoch [2/100], Step [19800/3934.5], Loss: 0.5885\n",
            "Epoch [2/100], Step [19900/3934.5], Loss: 0.4266\n",
            "Epoch [2/100], Step [20000/3934.5], Loss: 0.5828\n",
            "Epoch [2/100], Step [20100/3934.5], Loss: 0.3068\n",
            "Epoch [2/100], Step [20200/3934.5], Loss: 0.7042\n",
            "Epoch [2/100], Step [20300/3934.5], Loss: 0.4718\n",
            "Epoch [2/100], Step [20400/3934.5], Loss: 0.6008\n",
            "Epoch [2/100], Step [20500/3934.5], Loss: 0.4616\n",
            "Epoch [2/100], Step [20600/3934.5], Loss: 0.5745\n",
            "Epoch [2/100], Step [20700/3934.5], Loss: 0.2419\n",
            "Epoch [2/100], Step [20800/3934.5], Loss: 0.3095\n",
            "Epoch [2/100], Step [20900/3934.5], Loss: 0.4715\n",
            "Epoch [2/100], Step [21000/3934.5], Loss: 0.3625\n",
            "Epoch [2/100], Step [21100/3934.5], Loss: 0.5054\n",
            "Epoch [2/100], Step [21200/3934.5], Loss: 0.3830\n",
            "Epoch [2/100], Step [21300/3934.5], Loss: 0.5592\n",
            "Epoch [2/100], Step [21400/3934.5], Loss: 0.4641\n",
            "Epoch [2/100], Step [21500/3934.5], Loss: 0.7144\n",
            "Epoch [2/100], Step [21600/3934.5], Loss: 0.6397\n",
            "Epoch [2/100], Step [21700/3934.5], Loss: 0.3780\n",
            "Epoch [2/100], Step [21800/3934.5], Loss: 0.3037\n",
            "Epoch [2/100], Step [21900/3934.5], Loss: 0.3344\n",
            "Epoch [2/100], Step [22000/3934.5], Loss: 0.6120\n",
            "Epoch [2/100], Step [22100/3934.5], Loss: 0.3683\n",
            "Epoch [2/100], Step [22200/3934.5], Loss: 0.5055\n",
            "Epoch [2/100], Step [22300/3934.5], Loss: 0.4191\n",
            "Epoch [2/100], Step [22400/3934.5], Loss: 0.5991\n",
            "Epoch [2/100], Step [22500/3934.5], Loss: 0.3673\n",
            "Epoch [2/100], Step [22600/3934.5], Loss: 0.6136\n",
            "Epoch [2/100], Step [22700/3934.5], Loss: 0.3831\n",
            "Epoch [2/100], Step [22800/3934.5], Loss: 0.3757\n",
            "Epoch [2/100], Step [22900/3934.5], Loss: 0.3146\n",
            "Epoch [2/100], Step [23000/3934.5], Loss: 0.6041\n",
            "Epoch [2/100], Step [23100/3934.5], Loss: 0.6476\n",
            "Epoch [2/100], Step [23200/3934.5], Loss: 0.4236\n",
            "Epoch [2/100], Step [23300/3934.5], Loss: 0.4829\n",
            "Epoch [2/100], Step [23400/3934.5], Loss: 0.5806\n",
            "Epoch [2/100], Step [23500/3934.5], Loss: 0.5333\n",
            "Epoch [2/100], Step [23600/3934.5], Loss: 0.6147\n",
            "Epoch [2/100], Step [23700/3934.5], Loss: 0.4326\n",
            "Epoch [2/100], Step [23800/3934.5], Loss: 0.4410\n",
            "Epoch [2/100], Step [23900/3934.5], Loss: 0.5452\n",
            "Epoch [2/100], Step [24000/3934.5], Loss: 0.3676\n",
            "Epoch [2/100], Step [24100/3934.5], Loss: 0.4374\n",
            "Epoch [2/100], Step [24200/3934.5], Loss: 0.5105\n",
            "Epoch [2/100], Step [24300/3934.5], Loss: 0.5869\n",
            "Epoch [2/100], Step [24400/3934.5], Loss: 0.2851\n",
            "Epoch [2/100], Step [24500/3934.5], Loss: 0.6359\n",
            "Epoch [2/100], Step [24600/3934.5], Loss: 0.5051\n",
            "Epoch [2/100], Step [24700/3934.5], Loss: 0.4761\n",
            "Epoch [2/100], Step [24800/3934.5], Loss: 0.3607\n",
            "Epoch [2/100], Step [24900/3934.5], Loss: 0.4448\n",
            "Epoch [2/100], Step [25000/3934.5], Loss: 0.4476\n",
            "Epoch [2/100], Step [25100/3934.5], Loss: 0.2853\n",
            "Epoch [2/100], Step [25200/3934.5], Loss: 0.4405\n",
            "Epoch [2/100], Step [25300/3934.5], Loss: 0.4228\n",
            "Epoch [2/100], Step [25400/3934.5], Loss: 0.5001\n",
            "Epoch [2/100], Step [25500/3934.5], Loss: 0.4634\n",
            "Epoch [2/100], Step [25600/3934.5], Loss: 0.4079\n",
            "Epoch [2/100], Step [25700/3934.5], Loss: 0.2769\n",
            "Epoch [2/100], Step [25800/3934.5], Loss: 0.4449\n",
            "Epoch [2/100], Step [25900/3934.5], Loss: 0.4573\n",
            "Epoch [2/100], Step [26000/3934.5], Loss: 0.3498\n",
            "Epoch [2/100], Step [26100/3934.5], Loss: 0.4220\n",
            "Epoch [2/100], Step [26200/3934.5], Loss: 0.4821\n",
            "Epoch [2/100], Step [26300/3934.5], Loss: 0.3469\n",
            "Epoch [2/100], Step [26400/3934.5], Loss: 0.3743\n",
            "Epoch [2/100], Step [26500/3934.5], Loss: 0.3039\n",
            "Epoch [2/100], Step [26600/3934.5], Loss: 0.4378\n",
            "Epoch [2/100], Step [26700/3934.5], Loss: 0.4934\n",
            "Epoch [2/100], Step [26800/3934.5], Loss: 0.4716\n",
            "Epoch [2/100], Step [26900/3934.5], Loss: 0.5587\n",
            "Epoch [2/100], Step [27000/3934.5], Loss: 0.5727\n",
            "Epoch [2/100], Step [27100/3934.5], Loss: 0.4185\n",
            "Epoch [2/100], Step [27200/3934.5], Loss: 0.2737\n",
            "Epoch [2/100], Step [27300/3934.5], Loss: 0.3556\n",
            "Epoch [2/100], Step [27400/3934.5], Loss: 0.4065\n",
            "Epoch [2/100], Step [27500/3934.5], Loss: 0.3247\n",
            "Epoch [2/100], Step [27600/3934.5], Loss: 0.4928\n",
            "Epoch [2/100], Step [27700/3934.5], Loss: 0.5046\n",
            "Epoch [2/100], Step [27800/3934.5], Loss: 0.3962\n",
            "Epoch [2/100], Step [27900/3934.5], Loss: 0.5539\n",
            "Epoch [2/100], Step [28000/3934.5], Loss: 0.4544\n",
            "Epoch [2/100], Step [28100/3934.5], Loss: 0.6098\n",
            "Epoch [2/100], Step [28200/3934.5], Loss: 0.2833\n",
            "Epoch [2/100], Step [28300/3934.5], Loss: 0.6526\n",
            "Epoch [2/100], Step [28400/3934.5], Loss: 0.4723\n",
            "Epoch [2/100], Step [28500/3934.5], Loss: 0.2597\n",
            "Epoch [2/100], Step [28600/3934.5], Loss: 0.5776\n",
            "Epoch [2/100], Step [28700/3934.5], Loss: 0.2610\n",
            "Epoch [2/100], Step [28800/3934.5], Loss: 0.2292\n",
            "Epoch [2/100], Step [28900/3934.5], Loss: 0.4213\n",
            "Epoch [2/100], Step [29000/3934.5], Loss: 0.5428\n",
            "Epoch [2/100], Step [29100/3934.5], Loss: 0.4292\n",
            "Epoch [2/100], Step [29200/3934.5], Loss: 0.3346\n",
            "Epoch [2/100], Step [29300/3934.5], Loss: 0.4325\n",
            "Epoch [2/100], Step [29400/3934.5], Loss: 0.3407\n",
            "Epoch [2/100], Step [29500/3934.5], Loss: 0.3076\n",
            "Epoch [2/100], Step [29600/3934.5], Loss: 0.4812\n",
            "Epoch [2/100], Step [29700/3934.5], Loss: 0.4208\n",
            "Epoch [2/100], Step [29800/3934.5], Loss: 0.4247\n",
            "Epoch [2/100], Step [29900/3934.5], Loss: 0.2842\n",
            "Epoch [2/100], Step [30000/3934.5], Loss: 0.5352\n",
            "Epoch [2/100], Step [30100/3934.5], Loss: 0.5686\n",
            "Epoch [2/100], Step [30200/3934.5], Loss: 0.4047\n",
            "Epoch [2/100], Step [30300/3934.5], Loss: 0.2876\n",
            "Epoch [2/100], Step [30400/3934.5], Loss: 0.4776\n",
            "Epoch [2/100], Step [30500/3934.5], Loss: 0.4284\n",
            "Epoch [2/100], Step [30600/3934.5], Loss: 0.3924\n",
            "Epoch [2/100], Step [30700/3934.5], Loss: 0.4918\n",
            "Epoch [2/100], Step [30800/3934.5], Loss: 0.5033\n",
            "Epoch [2/100], Step [30900/3934.5], Loss: 0.2561\n",
            "Epoch [2/100], Step [31000/3934.5], Loss: 0.3266\n",
            "Epoch [2/100], Step [31100/3934.5], Loss: 0.5262\n",
            "Epoch [2/100], Step [31200/3934.5], Loss: 0.3555\n",
            "Epoch [2/100], Step [31300/3934.5], Loss: 0.3145\n",
            "Epoch [2/100], Step [31400/3934.5], Loss: 0.5074\n",
            "\n",
            "  Training epoch took: 2:40:23\n",
            "Running Validation...\n",
            "  Validation loss: 0.05\n",
            "\n",
            "======== Epoch 3 / 100 ========\n",
            "Epoch [3/100], Step [100/3934.5], Loss: 0.4990\n",
            "Epoch [3/100], Step [200/3934.5], Loss: 0.5008\n",
            "Epoch [3/100], Step [300/3934.5], Loss: 0.5212\n",
            "Epoch [3/100], Step [400/3934.5], Loss: 0.3483\n",
            "Epoch [3/100], Step [500/3934.5], Loss: 0.3799\n",
            "Epoch [3/100], Step [600/3934.5], Loss: 0.4129\n",
            "Epoch [3/100], Step [700/3934.5], Loss: 0.4631\n",
            "Epoch [3/100], Step [800/3934.5], Loss: 0.3022\n",
            "Epoch [3/100], Step [900/3934.5], Loss: 0.2616\n",
            "Early exit from training.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IspgrY9EvO59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}